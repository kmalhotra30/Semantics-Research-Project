{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to /Users/mefkov/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mefkov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mefkov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dataset import DatasetPreProcessor\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp = DatasetPreProcessor(corpus = 'semcor', first_time = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {}\n",
    "Params['split'] = 0.8 \n",
    "Params['windowSize'] = 5\n",
    "Params['batchSize'] = 64\n",
    "Params['wordEmbedding'] = dpp.embedding\n",
    "Params['wordEmbeddingSize'] = 300\n",
    "Params['sentenceEncoder'] = 'simple test'\n",
    "Params['filenameX'] = 'old/XBatches.pkl'\n",
    "Params['filenameSenses'] =  'old/SBatches.pkl'\n",
    "Params['filenameLength'] = 'old/lengthBatches.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp.get_train_validation_split(split = Params['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    \n",
    "    WordsToDisambiguate = set()\n",
    "    WordsSent = dict()\n",
    "    trainDocIds = dpp.doc_ids['train']\n",
    "    for trainDocId in trainDocIds:\n",
    "        for word in list(dpp.document[trainDocId]['wsd'].keys()):\n",
    "            WordsToDisambiguate.add(word)\n",
    "            WordsSent[word] = set()\n",
    "        \n",
    "    for trainDocId in trainDocIds:\n",
    "        for word in list(dpp.document[trainDocId]['wsd'].keys()):\n",
    "            for sent in dpp.document[trainDocId]['wsd'][word].keys():\n",
    "                WordsSent[word].add(sent)\n",
    "            \n",
    "    for word in list(WordsSent.keys()):\n",
    "        WordsSent[word] = list(WordsSent[word])\n",
    "    \n",
    "    WordsToDisambiguate = list(WordsToDisambiguate)\n",
    "    \n",
    "    return WordsSent, WordsToDisambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsSent, WordsToDisambiguate = prepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXSLpickle(WordsSent, WordsToDisambiguate, Params = Params, load_from_file = True, sensesList = training.senses):\n",
    "    \n",
    "    '''\n",
    "    WordsToDisambiguate = set()\n",
    "    WordsSent = dict()\n",
    "    trainDocIds = dpp.doc_ids['train']\n",
    "    for trainDocId in trainDocIds:\n",
    "        for word in list(dpp.document[trainDocId]['wsd'].keys()):\n",
    "            WordsToDisambiguate.add(word)\n",
    "            WordsSent[word] = set()\n",
    "        \n",
    "    for trainDocId in trainDocIds:\n",
    "        for word in list(dpp.document[trainDocId]['wsd'].keys()):\n",
    "            for sent in dpp.document[trainDocId]['wsd'][word].keys():\n",
    "                WordsSent[word].add(sent)\n",
    "            \n",
    "    for word in list(WordsSent.keys()):\n",
    "        WordsSent[word] = list(WordsSent[word])\n",
    "    \n",
    "    WordsToDisambiguate = list(WordsToDisambiguate)\n",
    "    '''\n",
    "    \n",
    "    windowT = Params['windowSize']\n",
    "    data_type = 'train'\n",
    "    \n",
    "    if load_from_file:\n",
    "    \n",
    "        keys = list(set(sensesList))\n",
    "        embed_len = Params['wordEmbeddingSize']\n",
    "        SenseVocab = dict.fromkeys(keys, (torch.zeros(1, embed_len), 0))\n",
    "    \n",
    "\n",
    "    XBatches = []\n",
    "    SBathes = []\n",
    "    lengthBatches = []\n",
    "    \n",
    "    if not(load_from_file):\n",
    "        for WordToDisambiguate in WordsToDisambiguate:\n",
    "\n",
    "            for sense in WordsSent[WordToDisambiguate]:\n",
    "                t = dpp.get_wsd_context(dataset_type = data_type, word = WordToDisambiguate, word_sense = sense, context_num = windowT, is_word = True, is_sentence = False, is_document = False)\n",
    "                for context in t['masked'][0]:\n",
    "                    t = [dpp.w2i[word] for word in context]\n",
    "                    XBatches.append(t)\n",
    "                    lengthBatches.append(np.count_nonzero(t))\n",
    "                    SBathes.append(sense)\n",
    "                    \n",
    "        keys = list(set([item for sublist in list(WordsSent.values()) for item in sublist]))\n",
    "        embed_len = Params['wordEmbeddingSize']\n",
    "        SenseVocab = dict.fromkeys(keys, (torch.zeros(1, embed_len), 0))\n",
    "        \n",
    "        with open(Params['filenameX'], 'wb') as f:\n",
    "               pickle.dump(XBatches, f)\n",
    "       \n",
    "        with open(Params['filenameSenses'], 'wb') as f:\n",
    "               pickle.dump(SBatches, f)\n",
    "       \n",
    "        with open(Params['filenameLength'], 'wb') as f:\n",
    "               pickle.dump(lengthBatches, f)\n",
    "                \n",
    "    return SenseVocab, XBatches, SBathes, lengthBatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#XBatches, SBathes, lengthBatches = createXSLpickle(WordsSent, WordsToDisambiguate, windowT = Params['windowSize'], data_type = 'train')\n",
    "\n",
    "#with open(Params['filenameX'], 'wb') as f:\n",
    "#       pickle.dump(XBatches, f)\n",
    "       \n",
    "#with open(Params['filenameSenses'], 'wb') as f:\n",
    "#       pickle.dump(SBatches, f)\n",
    "       \n",
    "#with open(Params['filenameLength'], 'wb') as f:\n",
    "#       pickle.dump(lengthBatches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, filenameX, filenameS, filenameL):\n",
    "        'Initialization'\n",
    "\n",
    "        with open(filenameL, 'rb') as f:\n",
    "            self.length = pickle.load(f)\n",
    "            \n",
    "        with open(filenameS, 'rb') as f:\n",
    "            self.senses = pickle.load(f)\n",
    "            \n",
    "        with open(filenameX, 'rb') as f:\n",
    "            self.X = np.array(pickle.load(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.senses)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        senses_idx = self.senses[index]\n",
    "        X_idx = self.X[index]\n",
    "        length_idx = self.length[index]\n",
    "\n",
    "        return (X_idx, senses_idx, length_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_vectors(data, senses, length, word_embedding = Params['wordEmbedding'], sentence_encoder = Params['sentenceEncoder']):\n",
    "    \n",
    "    '''\n",
    "    context_vectors.size(0) - batch size\n",
    "    context_vectors.size(1) - the context vector size\n",
    "    context_vectors.size(2) - the dim of word embeddings\n",
    "    '''\n",
    "    \n",
    "    data_torch = torch.LongTensor(data)\n",
    "    data_embedding = word_embedding(data_torch)\n",
    "    if sentence_encoder == 'simple test':\n",
    "        context_vectors = data_embedding.mean(dim = 1)\n",
    "    else: \n",
    "        context_vectors = sentence_encoder(data_embedding, length)\n",
    "    \n",
    "    return (context_vectors, senses, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense(context_vectors, senses, SenseVocab):\n",
    "    \n",
    "    senses_ = np.unique(senses)\n",
    "    for sense_ in senses_:\n",
    "        ind = torch.LongTensor([i for i,val in enumerate(senses) if val==sense_])\n",
    "        len1 = len(ind)\n",
    "        emb1 = context_vectors[ind, :].sum(dim = 0)\n",
    "        temp = SenseVocab[sense_] \n",
    "        emb2 = (temp[0]*temp[1] + emb1)\n",
    "        emb2 = emb2/(len1+temp[1]) # this is the weighted average\n",
    "        len2 = len1+temp[1]\n",
    "        SenseVocab[sense_] = (emb2, len2)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Dataset(Params['filenameX'], Params['filenameSenses'], Params['filenameLength'])\n",
    "training_generator = data.DataLoader(training, batch_size = Params['batchSize'], num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "SenseVocab, _, _, _  = createXSLpickle(WordsSent, WordsToDisambiguate, Params = Params, load_from_file = True, sensesList = training.senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSenseVocabulary(Vocab = SenseVocab, generator = training_generator):\n",
    "    \n",
    "    for batchesX, batchesSense, batchesLength in generator:\n",
    "        (context_vectors, senses, length) = get_context_vectors(batchesX, batchesSense, batchesLength)\n",
    "        get_sense(context_vectors, senses, Vocab)\n",
    "    \n",
    "    return SenseVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SenseVocab = createSenseVocabulary(Vocab = SenseVocab, generator = training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
